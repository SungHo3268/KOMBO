# Tokenization
<br/>

## 1. Stroke Tokenizer
* text: 테스트 문장입니다.
* tokens: ['ㄴ', '-', '-', 'ㅔ', '▂', 'ㅅ', 'ㅡ', '▂', 'ㄴ', '-', '-', 'ㅡ', '▁', 'ㅁ', 'ㅜ', 'ㄴ', '▂', 'ㅅ', '-', 'ㅏ', 'ㅇ', '▂', 'ㅇ', 'ㅣ', 'ㅁ', '-', '▂', 'ㄴ', 'ㅣ', '▂', 'ㄴ', '-', 'ㅏ', '▂', '.', '▂']
* detok: 테스트 문장입니다.
<br/>

## 2. Cji Tokenizer
* text: 테스트 문장입니다.
* tokens: ['ㅌ', 'ㆍ', 'ㅣ', 'ㅣ', '▂', 'ㅅ', 'ㅡ', '▂', 'ㅌ', 'ㅡ', '▁', 'ㅁ', 'ㅡ', 'ㆍ', 'ㄴ', '▂', 'ㅈ', 'ㅣ', 'ㆍ', 'ㅇ', '▂', 'ㅇ', 'ㅣ', 'ㅂ', '▂', 'ㄴ', 'ㅣ', '▂', 'ㄷ', 'ㅣ', 'ㆍ', '▂', '.', '▂']
* detok: 테스트 문장입니다.
<br/>

## 3. BTS Tokenizer
* text: 테스트 문장입니다.
* tokens: ['ㄴ', '-', '-', 'ㆍ', 'ㅣ', 'ㅣ', '▂', 'ㅅ', 'ㅡ', '▂', 'ㄴ', '-', '-', 'ㅡ', '▁', 'ㅁ', 'ㅡ', 'ㆍ', 'ㄴ', '▂', 'ㅅ', '-', 'ㅣ', 'ㆍ', 'ㅇ', '▂', 'ㅇ', 'ㅣ', 'ㅁ', '-', '▂', 'ㄴ', 'ㅣ', '▂', 'ㄴ', '-', 'ㅣ', 'ㆍ', '▂', '.', '▂']
* detok: 테스트 문장입니다.
<br/>

## 4. Jamo Tokenizer
* text: 테스트 문장입니다.
* tokens: ['ᄐ', 'ᅦ', 'ᄉ', 'ᅳ', 'ᄐ', 'ᅳ', '▁', 'ᄆ', 'ᅮ', 'ᆫ', 'ᄌ', 'ᅡ', 'ᆼ', 'ᄋ', 'ᅵ', 'ᆸ', 'ᄂ', 'ᅵ', 'ᄃ', 'ᅡ', '.']
* detok: 테스트 문장입니다.
<br/>

## 5. Character Tokenizer
* text: 테스트 문장입니다.
* tokens: ['테', '스', '트', '▁', '문', '장', '입', '니', '다', '.']
* detok: 테스트 문장입니다.
<br/>

## 6. Morpheme Tokenizer
* text: 테스트 문장입니다.
* tokens: ['테스트', '▃', '문장', '입니다', '.']
* detok: 테스트 문장입니다.
<br/>

## 7. Subword(SentencePiece) Tokenizer
* text: 테스트 문장입니다.
* tokens: ['▁테스트', '▁문장', '입니다', '.']
* detok: 테스트 문장입니다.
<br/>

## 8. Morpheme-aware Subword Tokenizer
* text: 테스트 문장입니다.
* tokens: ['▁테스트', '▃', '▁문장', '▁입니다', '▁.']
* detok: 테스트 문장입니다.
<br/>

## 9. Word Tokenizer
* text: 테스트 문장입니다.
* tokens: ['테스트', '문장입니다', '.']
* detok: 테스트 문장입니다 .
* <div>
    <code><B>[Notice]</B></code> Moses Tokenizer does not properly support Korean. Therefore, because the words are split based on spacing, the final punctuation point is not controlled. The space management code before the punctuation at the end of the sentence has been added and commented out, so you can use it by uncommenting that code.
</div>
<br/>

## 10. Stroke_var Tokenizer (w/ empty_token(▃) version of Stroke for KOMBO)
* text: 테스트 문장입니다.
* tokens: ['ㄴ', '-', '-', '▃', 'ㅔ', '▃', '▃', '▃', '▃', 'ㅅ', '▃', '▃', '▃', 'ㅡ', '▃', '▃', '▃', '▃', 'ㄴ', '-', '-', '▃', 'ㅡ', '▃', '▃', '▃', '▃', '▁', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃', 'ㅁ', '▃', '▃', '▃', 'ㅜ', 'ㄴ', '▃', '▃', '▃', 'ㅅ', '-', '▃', '▃', 'ㅏ', 'ㅇ', '▃', '▃', '▃', 'ㅇ', '▃', '▃', '▃', 'ㅣ', 'ㅁ', '-', '▃', '▃', 'ㄴ', '▃', '▃', '▃', 'ㅣ', '▃', '▃', '▃', '▃', 'ㄴ', '-', '▃', '▃', 'ㅏ', '▃', '▃', '▃', '▃', '.', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃']
* detok: 테스트 문장입니다.
<br/>

## 11. Cji_var Tokenizer (w/ empty_token(▃) version of Cji for KOMBO)
* text: 테스트 문장입니다.
* tokens: ['ㅌ', 'ㆍ', 'ㅣ', 'ㅣ', '▃', '▃', '▃', 'ㅅ', 'ㅡ', '▃', '▃', '▃', '▃', '▃', 'ㅌ', 'ㅡ', '▃', '▃', '▃', '▃', '▃', '▁', '▃', '▃', '▃', '▃', '▃', '▃', 'ㅁ', 'ㅡ', 'ㆍ', '▃', '▃', '▃', 'ㄴ', 'ㅈ', 'ㅣ', 'ㆍ', '▃', '▃', '▃', 'ㅇ', 'ㅇ', 'ㅣ', '▃', '▃', '▃', '▃', 'ㅂ', 'ㄴ', 'ㅣ', '▃', '▃', '▃', '▃', '▃', 'ㄷ', 'ㅣ', 'ㆍ', '▃', '▃', '▃', '▃', '.', '▃', '▃', '▃', '▃', '▃', '▃']
* detok: 테스트 문장입니다.
<br/>

## 12. BTS_var Tokenizer (w/ empty_token(▃) version of BTS for KOMBO)
* text: 테스트 문장입니다.
* tokens: ['ㄴ', '-', '-', '▃', 'ㆍ', 'ㅣ', 'ㅣ', '▃', '▃', '▃', '▃', '▃', '▃', 'ㅅ', '▃', '▃', '▃', 'ㅡ', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃', 'ㄴ', '-', '-', '▃', 'ㅡ', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▁', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃', 'ㅁ', '▃', '▃', '▃', 'ㅡ', 'ㆍ', '▃', '▃', '▃', 'ㄴ', '▃', '▃', '▃', 'ㅅ', '-', '▃', '▃', 'ㅣ', 'ㆍ', '▃', '▃', '▃', 'ㅇ', '▃', '▃', '▃', 'ㅇ', '▃', '▃', '▃', 'ㅣ', '▃', '▃', '▃', '▃', 'ㅁ', '-', '▃', '▃', 'ㄴ', '▃', '▃', '▃', 'ㅣ', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃', 'ㄴ', '-', '▃', '▃', 'ㅣ', 'ㆍ', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '.', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃', '▃']
* detok: 테스트 문장입니다.
<br/>

## 13. Jamo_distinct Tokenizer (w/ empty_token(▃) version of Jamo for KOMBO)
* text: 테스트 문장입니다.
* tokens: ['ᄐ', 'ᅦ', '▃', 'ᄉ', 'ᅳ', '▃', 'ᄐ', 'ᅳ', '▃', '▁', '▃', '▃', 'ᄆ', 'ᅮ', 'ᆫ', 'ᄌ', 'ᅡ', 'ᆼ', 'ᄋ', 'ᅵ', 'ᆸ', 'ᄂ', 'ᅵ', '▃', 'ᄃ', 'ᅡ', '▃', '.', '▃', '▃']
* detok: 테스트 문장입니다.
<br/>
